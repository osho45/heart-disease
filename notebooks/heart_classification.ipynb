{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Heart Classification\n",
        "\n",
        "This notebook demonstrates a complete ML lifecycle:\n",
        "1.  **Database Normalization**\n",
        "2.  **Data Loading**\n",
        "3.  **EDA & Preprocessing**\n",
        "4.  **Experiment Tracking**\n",
        "5.  **Model Selection**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sqlite3\n",
        "import os\n",
        "import joblib\n",
        "import json\n",
        "import mlflow\n",
        "import optuna\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.base import clone\n",
        "\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "\n",
        "DATA_DIR = '../Data'\n",
        "DB_PATH = os.path.join(DATA_DIR, 'heart.db')\n",
        "CSV_PATH = os.path.join(DATA_DIR, 'heart.csv')\n",
        "MODELS_DIR = '../models'\n",
        "\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Database Creation & Normalization\n",
        "\n",
        "We will read the raw CSV and convert it into a normalized SQLite schema.\n",
        "\n",
        "**Schema Design**:\n",
        "-   `patients`: `id` (generated), `age`, `sex`\n",
        "-   `lookup_cp`, `lookup_restecg`, `lookup_slope`, `lookup_thal`: Reference tables for categorical codes.\n",
        "-   `exams`: Foreign keys to patients and lookups, plus remaining measurements.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_normalized_db(csv_path, db_path):\n",
        "    if os.path.exists(db_path):\n",
        "        os.remove(db_path)\n",
        "    \n",
        "    conn = sqlite3.connect(db_path)\n",
        "    cursor = conn.cursor()\n",
        "    \n",
        "    \n",
        "    df = pd.read_csv(csv_path)\n",
        "    \n",
        "    df['patient_id'] = range(1, len(df) + 1)\n",
        "    \n",
        "\n",
        "    cp_values = sorted(df['cp'].unique())\n",
        "    cursor.execute(\"CREATE TABLE lookup_cp (cp_code INTEGER PRIMARY KEY, description TEXT)\")\n",
        "    cursor.executemany(\"INSERT INTO lookup_cp (cp_code, description) VALUES (?, ?)\", \n",
        "                       [(int(x), f\"cp_{x}\") for x in cp_values])\n",
        "                       \n",
        "    \n",
        "    restecg_values = sorted(df['restecg'].unique())\n",
        "    cursor.execute(\"CREATE TABLE lookup_restecg (restecg_code INTEGER PRIMARY KEY, description TEXT)\")\n",
        "    cursor.executemany(\"INSERT INTO lookup_restecg (restecg_code, description) VALUES (?, ?)\", \n",
        "                       [(int(x), f\"restecg_{x}\") for x in restecg_values])\n",
        "                       \n",
        "    \n",
        "    slope_values = sorted(df['slope'].unique())\n",
        "    cursor.execute(\"CREATE TABLE lookup_slope (slope_code INTEGER PRIMARY KEY, description TEXT)\")\n",
        "    cursor.executemany(\"INSERT INTO lookup_slope (slope_code, description) VALUES (?, ?)\", \n",
        "                       [(int(x), f\"slope_{x}\") for x in slope_values])\n",
        "                       \n",
        "    \n",
        "    thal_values = sorted(df['thal'].unique())\n",
        "    cursor.execute(\"CREATE TABLE lookup_thal (thal_code INTEGER PRIMARY KEY, description TEXT)\")\n",
        "    cursor.executemany(\"INSERT INTO lookup_thal (thal_code, description) VALUES (?, ?)\", \n",
        "                       [(int(x), f\"thal_{x}\") for x in thal_values])\n",
        "\n",
        "    \n",
        "    cursor.execute('''\n",
        "        CREATE TABLE patients (\n",
        "            patient_id INTEGER PRIMARY KEY,\n",
        "            age INTEGER,\n",
        "            sex INTEGER\n",
        "        )\n",
        "    ''')\n",
        "    patients_data = df[['patient_id', 'age', 'sex']].drop_duplicates()\n",
        "    patients_data.to_sql('patients', conn, if_exists='append', index=False)\n",
        "    \n",
        "    \n",
        "    cursor.execute('''\n",
        "        CREATE TABLE exams (\n",
        "            exam_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "            patient_id INTEGER,\n",
        "            cp INTEGER,\n",
        "            trestbps INTEGER,\n",
        "            chol INTEGER,\n",
        "            fbs INTEGER,\n",
        "            restecg INTEGER,\n",
        "            thalach INTEGER,\n",
        "            exang INTEGER,\n",
        "            oldpeak REAL,\n",
        "            slope INTEGER,\n",
        "            ca INTEGER,\n",
        "            thal INTEGER,\n",
        "            target INTEGER,\n",
        "            FOREIGN KEY(patient_id) REFERENCES patients(patient_id),\n",
        "            FOREIGN KEY(cp) REFERENCES lookup_cp(cp_code),\n",
        "            FOREIGN KEY(restecg) REFERENCES lookup_restecg(restecg_code),\n",
        "            FOREIGN KEY(slope) REFERENCES lookup_slope(slope_code),\n",
        "            FOREIGN KEY(thal) REFERENCES lookup_thal(thal_code)\n",
        "        )\n",
        "    ''')\n",
        "    \n",
        "    \n",
        "    exams_cols = ['patient_id', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', \n",
        "                  'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target']\n",
        "    exams_data = df[exams_cols]\n",
        "    exams_data.to_sql('exams', conn, if_exists='append', index=False)\n",
        "    \n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "    print(f\"Database created at {db_path}\")\n",
        "\n",
        "\n",
        "create_normalized_db(CSV_PATH, DB_PATH)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Loading \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_data_from_db(db_path):\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    \n",
        "    query = '''\n",
        "        SELECT \n",
        "            p.age, p.sex,\n",
        "            e.cp, e.trestbps, e.chol, e.fbs, e.restecg, \n",
        "            e.thalach, e.exang, e.oldpeak, e.slope, e.ca, e.thal, \n",
        "            e.target\n",
        "        FROM exams e\n",
        "        JOIN patients p ON e.patient_id = p.patient_id\n",
        "    '''\n",
        "    \n",
        "    df = pd.read_sql_query(query, conn)\n",
        "    conn.close()\n",
        "    return df\n",
        "\n",
        "item_df = load_data_from_db(DB_PATH)\n",
        "print(\"Data shape:\", item_df.shape)\n",
        "item_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. EDA & Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Class Balance\n",
        "print(\"Class Balance:\")\n",
        "print(item_df['target'].value_counts(normalize=True))\n",
        "\n",
        "# Summary Stats\n",
        "print(\"Summary Stats:\")\n",
        "print(item_df.describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define Preprocessing Pipeline\n",
        "\n",
        "X = item_df.drop('target', axis=1)\n",
        "y = item_df['target']\n",
        "\n",
        "# Categorical columns to encode\n",
        "cat_features = ['cp', 'restecg', 'slope', 'thal']\n",
        "# Numeric columns to scale (all others except sex, fbs, exang which are binary but scaling them is fine too usually, or pass through)\n",
        "# For simplicity, we'll scale all non-categorical features.\n",
        "num_features = [c for c in X.columns if c not in cat_features]\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), num_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Split Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, stratify=y, random_state=RANDOM_SEED)\n",
        "\n",
        "print(\"Train shape:\", X_train.shape)\n",
        "print(\"Test shape:\", X_test.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Experiment Loop (16 Runs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# pip install -q dagshub mlflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import dagshub\n",
        "dagshub.init(repo_owner=\"emmu9520\", repo_name=\"my-first-repo\", mlflow=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "models_registry = {\n",
        "    'LogisticRegression': LogisticRegression(random_state=RANDOM_SEED, max_iter=1000),\n",
        "    'RandomForest': RandomForestClassifier(random_state=RANDOM_SEED),\n",
        "    'SVC': SVC(random_state=RANDOM_SEED, probability=True),\n",
        "    'GradientBoosting': GradientBoostingClassifier(random_state=RANDOM_SEED)\n",
        "}\n",
        "\n",
        "\n",
        "def objective(trial, algo_name, X, y):\n",
        "    if algo_name == 'LogisticRegression':\n",
        "        C = trial.suggest_loguniform('C', 1e-4, 1e2)\n",
        "        model = LogisticRegression(C=C, random_state=RANDOM_SEED, max_iter=1000)\n",
        "    elif algo_name == 'RandomForest':\n",
        "        n_estimators = trial.suggest_int('n_estimators', 10, 200)\n",
        "        max_depth = trial.suggest_int('max_depth', 2, 20)\n",
        "        model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=RANDOM_SEED)\n",
        "    elif algo_name == 'SVC':\n",
        "        C = trial.suggest_loguniform('C', 1e-3, 1e2)\n",
        "        gamma = trial.suggest_loguniform('gamma', 1e-3, 1e2)\n",
        "        model = SVC(C=C, gamma=gamma, random_state=RANDOM_SEED, probability=True)\n",
        "    elif algo_name == 'GradientBoosting':\n",
        "        n_estimators = trial.suggest_int('n_estimators', 10, 200)\n",
        "        learning_rate = trial.suggest_loguniform('learning_rate', 0.001, 0.1)\n",
        "        max_depth = trial.suggest_int('max_depth', 2, 10)\n",
        "        model = GradientBoostingClassifier(n_estimators=n_estimators, learning_rate=learning_rate, max_depth=max_depth, random_state=RANDOM_SEED)\n",
        "    else:\n",
        "        return 0\n",
        "        \n",
        "    score = cross_val_score(model, X, y, cv=3, scoring='f1').mean()\n",
        "    return score\n",
        "\n",
        "\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "\n",
        "experiment_results = []\n",
        "mlflow_experiment_name = \"heart_classification_16_runs\"\n",
        "mlflow.set_experiment(mlflow_experiment_name)\n",
        "\n",
        "\n",
        "algorithms = ['LogisticRegression', 'RandomForest', 'SVC', 'GradientBoosting']\n",
        "pca_flags = [False, True]\n",
        "tuning_flags = [False, True]\n",
        "\n",
        "run_count = 0\n",
        "\n",
        "for algo in algorithms:\n",
        "    for use_pca in pca_flags:\n",
        "        for use_tuning in tuning_flags:\n",
        "            run_count += 1\n",
        "            run_name = f\"{algo}_{'PCA' if use_pca else 'NoPCA'}_{'Tuned' if use_tuning else 'Default'}\"\n",
        "            print(f\"Running scenario {run_count}/16: {run_name}\")\n",
        "            \n",
        "            with mlflow.start_run(run_name=run_name):\n",
        "                \n",
        "                steps = [('preprocessor', preprocessor)]\n",
        "                \n",
        "                if use_pca:\n",
        "                    \n",
        "                    steps.append(('pca', PCA(n_components=0.95, random_state=RANDOM_SEED)))\n",
        "                \n",
        "                \n",
        "                prep_pipe = Pipeline(steps)\n",
        "                X_train_transformed = prep_pipe.fit_transform(X_train, y_train)\n",
        "                \n",
        "                \n",
        "                best_params = {}\n",
        "                clf = None\n",
        "                \n",
        "                if use_tuning:\n",
        "                    study = optuna.create_study(direction='maximize')\n",
        "                    study.optimize(lambda trial: objective(trial, algo, X_train_transformed, y_train), n_trials=10)\n",
        "                    best_params = study.best_params\n",
        "                    \n",
        "                    if algo == 'LogisticRegression':\n",
        "                        clf = LogisticRegression(**best_params, random_state=RANDOM_SEED, max_iter=1000)\n",
        "                    elif algo == 'RandomForest':\n",
        "                        clf = RandomForestClassifier(**best_params, random_state=RANDOM_SEED)\n",
        "                    elif algo == 'SVC':\n",
        "                        clf = SVC(**best_params, random_state=RANDOM_SEED, probability=True)\n",
        "                    elif algo == 'GradientBoosting':\n",
        "                        clf = GradientBoostingClassifier(**best_params, random_state=RANDOM_SEED)\n",
        "                else:\n",
        "                    clf = clone(models_registry[algo])\n",
        "                    best_params = \"default\"\n",
        "\n",
        "                \n",
        "                full_pipeline = Pipeline(steps + [('classifier', clf)])\n",
        "                \n",
        "                full_pipeline.fit(X_train, y_train)\n",
        "                \n",
        "                \n",
        "                y_pred = full_pipeline.predict(X_test)\n",
        "                f1 = f1_score(y_test, y_pred)\n",
        "                acc = accuracy_score(y_test, y_pred)\n",
        "                \n",
        "                print(f\"  --> F1: {f1:.4f}\")\n",
        "                \n",
        "                \n",
        "                mlflow.log_param(\"algorithm\", algo)\n",
        "                mlflow.log_param(\"use_pca\", use_pca)\n",
        "                mlflow.log_param(\"use_tuning\", use_tuning)\n",
        "                mlflow.log_params(best_params if isinstance(best_params, dict) else {\"params\": \"default\"})\n",
        "                \n",
        "                mlflow.log_metric(\"f1_score\", f1)\n",
        "                mlflow.log_metric(\"accuracy\", acc)\n",
        "                \n",
        "                \n",
        "                model_filename = f\"model_{run_name}.joblib\"\n",
        "                joblib.dump(full_pipeline, model_filename)\n",
        "                mlflow.log_artifact(model_filename)\n",
        "                \n",
        "               \n",
        "                metrics = {\"f1_score\": f1, \"accuracy\": acc}\n",
        "                metrics_filename = f\"metrics_{run_name}.json\"\n",
        "                with open(metrics_filename, \"w\") as f:\n",
        "                    json.dump(metrics, f)\n",
        "                mlflow.log_artifact(metrics_filename)\n",
        "                \n",
        "                \n",
        "                meta = {\n",
        "                    \"run_name\": run_name,\n",
        "                    \"algorithm\": algo,\n",
        "                    \"use_pca\": use_pca,\n",
        "                    \"use_tuning\": use_tuning,\n",
        "                    \"best_params\": best_params\n",
        "                }\n",
        "                meta_filename = f\"metadata_{run_name}.json\"\n",
        "                with open(meta_filename, \"w\") as f:\n",
        "                    json.dump(meta, f)\n",
        "                mlflow.log_artifact(meta_filename)\n",
        "                \n",
        "                \n",
        "                os.remove(model_filename)\n",
        "                os.remove(metrics_filename)\n",
        "                os.remove(meta_filename)\n",
        "                \n",
        "                \n",
        "                experiment_results.append({\n",
        "                    \"run_name\": run_name,\n",
        "                    \"f1\": f1,\n",
        "                    \"model\": full_pipeline,\n",
        "                    \"metadata\": meta\n",
        "                })\n",
        "\n",
        "print(\"All experiments completed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Best Model Selection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "best_run = max(experiment_results, key=lambda x: x['f1'])\n",
        "\n",
        "print(f\"Best Run: {best_run['run_name']} with F1: {best_run['f1']:.4f}\")\n",
        "\n",
        "\n",
        "best_model_path = os.path.join(MODELS_DIR, \"best_model.joblib\")\n",
        "joblib.dump(best_run['model'], best_model_path)\n",
        "\n",
        "\n",
        "best_meta_path = os.path.join(MODELS_DIR, \"best_model_metadata.json\")\n",
        "with open(best_meta_path, \"w\") as f:\n",
        "    json.dump(best_run['metadata'], f)\n",
        "\n",
        "print(f\"Saved best model to {best_model_path}\")\n",
        "print(f\"Saved best metadata to {best_meta_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
