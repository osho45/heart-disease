{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Heart Classification End-to-End\n",
        "\n",
        "This notebook demonstrates a complete ML lifecycle:\n",
        "1.  **Database Normalization**: Loading CSV data into a normalized SQLite database.\n",
        "2.  **Data Loading**: Reconstructing the dataset via SQL.\n",
        "3.  **EDA & Preprocessing**: Analyzing class balance and building a pipeline.\n",
        "4.  **Experiment Tracking**: Running 16 experiments (Algorithm x PCA x Tuning) with MLflow.\n",
        "5.  **Model Selection**: Saving the best model based on F1 score.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\emman\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sqlite3\n",
        "import os\n",
        "import joblib\n",
        "import json\n",
        "import mlflow\n",
        "import optuna\n",
        "\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.base import clone\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "# Define paths\n",
        "DATA_DIR = '../Data'\n",
        "DB_PATH = os.path.join(DATA_DIR, 'heart.db')\n",
        "CSV_PATH = os.path.join(DATA_DIR, 'heart.csv')\n",
        "MODELS_DIR = '../models'\n",
        "\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Database Creation & Normalization\n",
        "\n",
        "We will read the raw CSV and convert it into a normalized SQLite schema.\n",
        "\n",
        "**Schema Design**:\n",
        "-   `patients`: `id` (generated), `age`, `sex`\n",
        "-   `lookup_cp`, `lookup_restecg`, `lookup_slope`, `lookup_thal`: Reference tables for categorical codes.\n",
        "-   `exams`: Foreign keys to patients and lookups, plus remaining measurements.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Database created at ../Data\\heart.db\n"
          ]
        }
      ],
      "source": [
        "def create_normalized_db(csv_path, db_path):\n",
        "    if os.path.exists(db_path):\n",
        "        os.remove(db_path)\n",
        "    \n",
        "    conn = sqlite3.connect(db_path)\n",
        "    cursor = conn.cursor()\n",
        "    \n",
        "    # 1. Read Raw Data\n",
        "    df = pd.read_csv(csv_path)\n",
        "    \n",
        "    # Generate Patient IDs (assuming one row per patient for this dataset)\n",
        "    df['patient_id'] = range(1, len(df) + 1)\n",
        "    \n",
        "    # 2. Extract Lookup Tables\n",
        "    # CP\n",
        "    cp_values = sorted(df['cp'].unique())\n",
        "    cursor.execute(\"CREATE TABLE lookup_cp (cp_code INTEGER PRIMARY KEY, description TEXT)\")\n",
        "    cursor.executemany(\"INSERT INTO lookup_cp (cp_code, description) VALUES (?, ?)\", \n",
        "                       [(int(x), f\"cp_{x}\") for x in cp_values])\n",
        "                       \n",
        "    # RestECG\n",
        "    restecg_values = sorted(df['restecg'].unique())\n",
        "    cursor.execute(\"CREATE TABLE lookup_restecg (restecg_code INTEGER PRIMARY KEY, description TEXT)\")\n",
        "    cursor.executemany(\"INSERT INTO lookup_restecg (restecg_code, description) VALUES (?, ?)\", \n",
        "                       [(int(x), f\"restecg_{x}\") for x in restecg_values])\n",
        "                       \n",
        "    # Slope\n",
        "    slope_values = sorted(df['slope'].unique())\n",
        "    cursor.execute(\"CREATE TABLE lookup_slope (slope_code INTEGER PRIMARY KEY, description TEXT)\")\n",
        "    cursor.executemany(\"INSERT INTO lookup_slope (slope_code, description) VALUES (?, ?)\", \n",
        "                       [(int(x), f\"slope_{x}\") for x in slope_values])\n",
        "                       \n",
        "    # Thal\n",
        "    thal_values = sorted(df['thal'].unique())\n",
        "    cursor.execute(\"CREATE TABLE lookup_thal (thal_code INTEGER PRIMARY KEY, description TEXT)\")\n",
        "    cursor.executemany(\"INSERT INTO lookup_thal (thal_code, description) VALUES (?, ?)\", \n",
        "                       [(int(x), f\"thal_{x}\") for x in thal_values])\n",
        "\n",
        "    # 3. Create Patients Table\n",
        "    # Storing Age and Sex separately as requested\n",
        "    cursor.execute('''\n",
        "        CREATE TABLE patients (\n",
        "            patient_id INTEGER PRIMARY KEY,\n",
        "            age INTEGER,\n",
        "            sex INTEGER\n",
        "        )\n",
        "    ''')\n",
        "    patients_data = df[['patient_id', 'age', 'sex']].drop_duplicates()\n",
        "    patients_data.to_sql('patients', conn, if_exists='append', index=False)\n",
        "    \n",
        "    # 4. Create Exams Table (The Main Table)\n",
        "    # References patients and lookups\n",
        "    cursor.execute('''\n",
        "        CREATE TABLE exams (\n",
        "            exam_id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "            patient_id INTEGER,\n",
        "            cp INTEGER,\n",
        "            trestbps INTEGER,\n",
        "            chol INTEGER,\n",
        "            fbs INTEGER,\n",
        "            restecg INTEGER,\n",
        "            thalach INTEGER,\n",
        "            exang INTEGER,\n",
        "            oldpeak REAL,\n",
        "            slope INTEGER,\n",
        "            ca INTEGER,\n",
        "            thal INTEGER,\n",
        "            target INTEGER,\n",
        "            FOREIGN KEY(patient_id) REFERENCES patients(patient_id),\n",
        "            FOREIGN KEY(cp) REFERENCES lookup_cp(cp_code),\n",
        "            FOREIGN KEY(restecg) REFERENCES lookup_restecg(restecg_code),\n",
        "            FOREIGN KEY(slope) REFERENCES lookup_slope(slope_code),\n",
        "            FOREIGN KEY(thal) REFERENCES lookup_thal(thal_code)\n",
        "        )\n",
        "    ''')\n",
        "    \n",
        "    # Prepare exams data\n",
        "    exams_cols = ['patient_id', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', \n",
        "                  'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'target']\n",
        "    exams_data = df[exams_cols]\n",
        "    exams_data.to_sql('exams', conn, if_exists='append', index=False)\n",
        "    \n",
        "    conn.commit()\n",
        "    conn.close()\n",
        "    print(f\"Database created at {db_path}\")\n",
        "\n",
        "# Run the creation function\n",
        "create_normalized_db(CSV_PATH, DB_PATH)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Loading (Reconstruction)\n",
        "\n",
        "We reconstruct the training DataFrame by joining the normalized tables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data shape: (1025, 14)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>cp</th>\n",
              "      <th>trestbps</th>\n",
              "      <th>chol</th>\n",
              "      <th>fbs</th>\n",
              "      <th>restecg</th>\n",
              "      <th>thalach</th>\n",
              "      <th>exang</th>\n",
              "      <th>oldpeak</th>\n",
              "      <th>slope</th>\n",
              "      <th>ca</th>\n",
              "      <th>thal</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>52</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>125</td>\n",
              "      <td>212</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>168</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>53</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>140</td>\n",
              "      <td>203</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>155</td>\n",
              "      <td>1</td>\n",
              "      <td>3.1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>145</td>\n",
              "      <td>174</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>125</td>\n",
              "      <td>1</td>\n",
              "      <td>2.6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>61</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>148</td>\n",
              "      <td>203</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>161</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>62</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>138</td>\n",
              "      <td>294</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>106</td>\n",
              "      <td>0</td>\n",
              "      <td>1.9</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
              "0   52    1   0       125   212    0        1      168      0      1.0      2   \n",
              "1   53    1   0       140   203    1        0      155      1      3.1      0   \n",
              "2   70    1   0       145   174    0        1      125      1      2.6      0   \n",
              "3   61    1   0       148   203    0        1      161      0      0.0      2   \n",
              "4   62    0   0       138   294    1        1      106      0      1.9      1   \n",
              "\n",
              "   ca  thal  target  \n",
              "0   2     3       0  \n",
              "1   0     3       0  \n",
              "2   0     3       0  \n",
              "3   1     3       0  \n",
              "4   3     2       0  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def load_data_from_db(db_path):\n",
        "    conn = sqlite3.connect(db_path)\n",
        "    \n",
        "    query = '''\n",
        "        SELECT \n",
        "            p.age, p.sex,\n",
        "            e.cp, e.trestbps, e.chol, e.fbs, e.restecg, \n",
        "            e.thalach, e.exang, e.oldpeak, e.slope, e.ca, e.thal, \n",
        "            e.target\n",
        "        FROM exams e\n",
        "        JOIN patients p ON e.patient_id = p.patient_id\n",
        "    '''\n",
        "    \n",
        "    df = pd.read_sql_query(query, conn)\n",
        "    conn.close()\n",
        "    return df\n",
        "\n",
        "item_df = load_data_from_db(DB_PATH)\n",
        "print(\"Data shape:\", item_df.shape)\n",
        "item_df.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. EDA & Preprocessing\n",
        "\n",
        "Basic checks and pipeline construction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 6) (444375797.py, line 6)",
          "output_type": "error",
          "traceback": [
            "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mprint(\"\u001b[39m\n          ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m unterminated string literal (detected at line 6)\n"
          ]
        }
      ],
      "source": [
        "# Class Balance\n",
        "print(\"Class Balance:\")\n",
        "print(item_df['target'].value_counts(normalize=True))\n",
        "\n",
        "# Summary Stats\n",
        "print(\"\n",
        "Summary Stats:\")\n",
        "print(item_df.describe())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define Preprocessing Pipeline\n",
        "\n",
        "X = item_df.drop('target', axis=1)\n",
        "y = item_df['target']\n",
        "\n",
        "# Categorical columns to encode\n",
        "cat_features = ['cp', 'restecg', 'slope', 'thal']\n",
        "# Numeric columns to scale (all others except sex, fbs, exang which are binary but scaling them is fine too usually, or pass through)\n",
        "# For simplicity, we'll scale all non-categorical features.\n",
        "num_features = [c for c in X.columns if c not in cat_features]\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), num_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore'), cat_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Split Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, stratify=y, random_state=RANDOM_SEED)\n",
        "\n",
        "print(\"Train shape:\", X_train.shape)\n",
        "print(\"Test shape:\", X_test.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Experiment Loop (16 Runs)\n",
        "\n",
        "Algorithms: LogReg, RF, SVC, GBM.\n",
        "Conditions: +/- PCA, +/- Optuna.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define Base Models\n",
        "models_registry = {\n",
        "    'LogisticRegression': LogisticRegression(random_state=RANDOM_SEED, max_iter=1000),\n",
        "    'RandomForest': RandomForestClassifier(random_state=RANDOM_SEED),\n",
        "    'SVC': SVC(random_state=RANDOM_SEED, probability=True),\n",
        "    'GradientBoosting': GradientBoostingClassifier(random_state=RANDOM_SEED)\n",
        "}\n",
        "\n",
        "# Optuna Objective Functions\n",
        "def objective(trial, algo_name, X, y):\n",
        "    if algo_name == 'LogisticRegression':\n",
        "        C = trial.suggest_loguniform('C', 1e-4, 1e2)\n",
        "        model = LogisticRegression(C=C, random_state=RANDOM_SEED, max_iter=1000)\n",
        "    elif algo_name == 'RandomForest':\n",
        "        n_estimators = trial.suggest_int('n_estimators', 10, 200)\n",
        "        max_depth = trial.suggest_int('max_depth', 2, 20)\n",
        "        model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=RANDOM_SEED)\n",
        "    elif algo_name == 'SVC':\n",
        "        C = trial.suggest_loguniform('C', 1e-3, 1e2)\n",
        "        gamma = trial.suggest_loguniform('gamma', 1e-3, 1e2)\n",
        "        model = SVC(C=C, gamma=gamma, random_state=RANDOM_SEED, probability=True)\n",
        "    elif algo_name == 'GradientBoosting':\n",
        "        n_estimators = trial.suggest_int('n_estimators', 10, 200)\n",
        "        learning_rate = trial.suggest_loguniform('learning_rate', 0.001, 0.1)\n",
        "        max_depth = trial.suggest_int('max_depth', 2, 10)\n",
        "        model = GradientBoostingClassifier(n_estimators=n_estimators, learning_rate=learning_rate, max_depth=max_depth, random_state=RANDOM_SEED)\n",
        "    else:\n",
        "        return 0\n",
        "        \n",
        "    score = cross_val_score(model, X, y, cv=3, scoring='f1').mean()\n",
        "    return score\n",
        "\n",
        "# Helper to capture Optuna logs to prevent clutter\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "\n",
        "experiment_results = []\n",
        "mlflow_experiment_name = \"heart_classification_16_runs\"\n",
        "mlflow.set_experiment(mlflow_experiment_name)\n",
        "\n",
        "# Ensure MLflow tracking URI is set (can be done via env vars outside notebook, or explicitly here if passed)\n",
        "# os.environ[\"MLFLOW_TRACKING_URI\"] = \"...\" \n",
        "\n",
        "# THE LOOP\n",
        "algorithms = ['LogisticRegression', 'RandomForest', 'SVC', 'GradientBoosting']\n",
        "pca_flags = [False, True]\n",
        "tuning_flags = [False, True]\n",
        "\n",
        "run_count = 0\n",
        "\n",
        "for algo in algorithms:\n",
        "    for use_pca in pca_flags:\n",
        "        for use_tuning in tuning_flags:\n",
        "            run_count += 1\n",
        "            run_name = f\"{algo}_{'PCA' if use_pca else 'NoPCA'}_{'Tuned' if use_tuning else 'Default'}\"\n",
        "            print(f\"Running scenario {run_count}/16: {run_name}\")\n",
        "            \n",
        "            with mlflow.start_run(run_name=run_name):\n",
        "                # 1. Build Preprocessing Step of Pipeline\n",
        "                steps = [('preprocessor', preprocessor)]\n",
        "                \n",
        "                if use_pca:\n",
        "                    # We pick n_components=0.95 (variance explained) as a sensible pivot for PCA\n",
        "                    steps.append(('pca', PCA(n_components=0.95, random_state=RANDOM_SEED)))\n",
        "                \n",
        "                # Combine preprocessing so we can feed it to Optuna if needed (Optuna needs to CV on transformed data or use pipeline)\n",
        "                # To keep it simple + correct: We used a Pipeline for the final model. \n",
        "                # For Optuna optimization, we'll optimize the CLASSIFIER params.\n",
        "                # We should pre-transform X_train to speed up Optuna, or put the whole thing in a pipeline inside CV.\n",
        "                # Let's use pre-transformed X for Optuna to save time.\n",
        "                \n",
        "                prep_pipe = Pipeline(steps)\n",
        "                X_train_transformed = prep_pipe.fit_transform(X_train, y_train)\n",
        "                \n",
        "                # 2. Determine Classifier & Params\n",
        "                best_params = {}\n",
        "                clf = None\n",
        "                \n",
        "                if use_tuning:\n",
        "                    study = optuna.create_study(direction='maximize')\n",
        "                    study.optimize(lambda trial: objective(trial, algo, X_train_transformed, y_train), n_trials=10) # 10 trials for speed\n",
        "                    best_params = study.best_params\n",
        "                    # Instantiate model with best params\n",
        "                    if algo == 'LogisticRegression':\n",
        "                        clf = LogisticRegression(**best_params, random_state=RANDOM_SEED, max_iter=1000)\n",
        "                    elif algo == 'RandomForest':\n",
        "                        clf = RandomForestClassifier(**best_params, random_state=RANDOM_SEED)\n",
        "                    elif algo == 'SVC':\n",
        "                        clf = SVC(**best_params, random_state=RANDOM_SEED, probability=True)\n",
        "                    elif algo == 'GradientBoosting':\n",
        "                        clf = GradientBoostingClassifier(**best_params, random_state=RANDOM_SEED)\n",
        "                else:\n",
        "                    clf = clone(models_registry[algo])\n",
        "                    best_params = \"default\"\n",
        "\n",
        "                # 3. Create Final Pipeline & Fit\n",
        "                # We need to rebuild the full pipeline (Prep [+ PCA] + Classifier)\n",
        "                # We reuse the `steps` list which has prep and maybe pca\n",
        "                full_pipeline = Pipeline(steps + [('classifier', clf)])\n",
        "                \n",
        "                full_pipeline.fit(X_train, y_train)\n",
        "                \n",
        "                # 4. Evaluate\n",
        "                y_pred = full_pipeline.predict(X_test)\n",
        "                f1 = f1_score(y_test, y_pred)\n",
        "                acc = accuracy_score(y_test, y_pred)\n",
        "                \n",
        "                print(f\"  --> F1: {f1:.4f}\")\n",
        "                \n",
        "                # 5. Log to MLflow\n",
        "                mlflow.log_param(\"algorithm\", algo)\n",
        "                mlflow.log_param(\"use_pca\", use_pca)\n",
        "                mlflow.log_param(\"use_tuning\", use_tuning)\n",
        "                mlflow.log_params(best_params if isinstance(best_params, dict) else {\"params\": \"default\"})\n",
        "                \n",
        "                mlflow.log_metric(\"f1_score\", f1)\n",
        "                mlflow.log_metric(\"accuracy\", acc)\n",
        "                \n",
        "                # Save Artifacts locally first then log\n",
        "                # Model\n",
        "                model_filename = f\"model_{run_name}.joblib\"\n",
        "                joblib.dump(full_pipeline, model_filename)\n",
        "                mlflow.log_artifact(model_filename)\n",
        "                \n",
        "                # Metrics JSON\n",
        "                metrics = {\"f1_score\": f1, \"accuracy\": acc}\n",
        "                metrics_filename = f\"metrics_{run_name}.json\"\n",
        "                with open(metrics_filename, \"w\") as f:\n",
        "                    json.dump(metrics, f)\n",
        "                mlflow.log_artifact(metrics_filename)\n",
        "                \n",
        "                # Metadata JSON\n",
        "                meta = {\n",
        "                    \"run_name\": run_name,\n",
        "                    \"algorithm\": algo,\n",
        "                    \"use_pca\": use_pca,\n",
        "                    \"use_tuning\": use_tuning,\n",
        "                    \"best_params\": best_params\n",
        "                }\n",
        "                meta_filename = f\"metadata_{run_name}.json\"\n",
        "                with open(meta_filename, \"w\") as f:\n",
        "                    json.dump(meta, f)\n",
        "                mlflow.log_artifact(meta_filename)\n",
        "                \n",
        "                # Cleanup local files\n",
        "                os.remove(model_filename)\n",
        "                os.remove(metrics_filename)\n",
        "                os.remove(meta_filename)\n",
        "                \n",
        "                # Store in results list for Final Selection\n",
        "                experiment_results.append({\n",
        "                    \"run_name\": run_name,\n",
        "                    \"f1\": f1,\n",
        "                    \"model\": full_pipeline,\n",
        "                    \"metadata\": meta\n",
        "                })\n",
        "\n",
        "print(\"All experiments completed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Best Model Selection\n",
        "\n",
        "Identifying the run with the highest F1 score and saving it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find best run\n",
        "best_run = max(experiment_results, key=lambda x: x['f1'])\n",
        "\n",
        "print(f\"Best Run: {best_run['run_name']} with F1: {best_run['f1']:.4f}\")\n",
        "\n",
        "# Save Best Model\n",
        "best_model_path = os.path.join(MODELS_DIR, \"best_model.joblib\")\n",
        "joblib.dump(best_run['model'], best_model_path)\n",
        "\n",
        "# Save Best Metadata\n",
        "best_meta_path = os.path.join(MODELS_DIR, \"best_model_metadata.json\")\n",
        "with open(best_meta_path, \"w\") as f:\n",
        "    json.dump(best_run['metadata'], f)\n",
        "\n",
        "print(f\"Saved best model to {best_model_path}\")\n",
        "print(f\"Saved best metadata to {best_meta_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
